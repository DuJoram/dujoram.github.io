<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
		<link rel="stylesheet" type="text/css" href="/css/reset.css">
		<link rel="stylesheet" type="text/css" href="/css/main.css">
		<title>On Artificial Superintelligence</title>
		<meta name="generator" content="Hugo 0.31.1" />
	</head>
	<body>
		<header>
			<div>
				<h1><a class="header-title" href="/">Joram S. Liebeskind</a></h1>
				<h2 id="subtitle">CS Student and Science Nerd</h2>
			</div>
			<nav>
				<ul>
					
					<li><a href="/">Blog</a>
					
					<li><a href="/about/">About</a>
					
				</ul>
			</nav>
		</header>

		<main class="markdown">
			<h1 class="post-title">On Artificial Superintelligence</h1>
			<div class="post-date">Posted on: 2017.10.02</div>
			<p>Artificial intelligence might just be the next big thing that the human species will come up with. It might also be the last thing that it will ever create. Or will ever do. We might all die and everything will be over. Alternatively, we might just get lucky and AI will turn us into the immortal multiplanetary species that we all (or at least some of us) dream of.
</p>

<h1 id="disclaimer">Disclaimer</h1>

<p>At the time of writing, I am far from being an expert on this topic and everything you&rsquo;re about to read is pure speculation, mostly based on a two-part blog post series with the titles &ldquo;The AI Revolution: The Road to Superintelligence&rdquo; and &ldquo;The AI Revolution: Our Immortality or Extinction&rdquo; by Tim Urban [<a href="#URBT00">URBT00</a>, <a href="#URBT01">URBT01</a>], as well as an article from AI Magazine with the title &ldquo;Cognitive Orthoses: Toward Human-Centered AI&rdquo; <a href="#FORK00">[FORK00]</a>.</p>

<p>I wrote this blog post in the context of an AI class assignement, which consisted of, in a first step reading the referenced material and thereafter draw conclusion in the form of a post.</p>

<h1 id="simulated-brains">Simulated Brains</h1>

<p>Progress increases more and more each day. Many fields in science experience an exponential growth. It is hard to understand what effects this has on our daily lives and even more challanging to imagine how this will effect our future. Arguably the most important development is the increase in computational capabilities. According to Tim Urban, computational power equivalent to a human brains ability, will be widely spread and affordable in the not too distant future.
Reaching this milestone will enable humans to simulate brains.
Then the question is, how to create an Artificial General Intelligence (AGI). That is, an AI which can absorb and interprete new knowledge, connect it with previously learned knowledge and apply it to arbitrary problems, which were either imposed onto itself or by other humans. For an AI to qualify as an AGI, this process has to reach a quality roughly equivalent to what the human brain is capable of achieving.
In the process of gathering new knowledge, it is only reasonable for an AGI to improve itself. This implies that it would rapidly reach a point, where it would be capable of mastering tasks and developing technologies far beyond our understanding at a rate which we couldn&rsquo;t keep up with. We&rsquo;d have an Artificial Super Intelligence (ASI), which created itself, simply because an AGI strives to become more intelligent. <a href="#URBT00">[URBT00]</a>
This is a very rough summary of Tim Urbans blog posts and only touches the surface, following I want to focus on my thoughts on different aspects of the scenario.</p>

<h1 id="questions-without-answers">Questions Without Answers</h1>

<p>In this section I want to discuss the idea of consciousness and human-like behaviour in AI system. Or rather, what the lack thereof implies.</p>

<p>At this point, I would also like to remind you of the &ldquo;Disclaimer&rdquo; section at the beginning of this blog post. I am in no way qualified to talk about the following topics and therefore statements might be completely inaccurate and/or just wrong.</p>

<h2 id="will-ai-have-consciousness">Will AI have consciousness?</h2>

<p>Firstly, we&rsquo;d need to understand what consciousness is, for how can we make a statement about how to create something if we don&rsquo;t know how it works. (Though this claim stands in direct contradiction to how nature works, where the most intelligent things are created with very little, if any, intellectual thought.)</p>

<p>Another thing is the fundamental difference between how the human brain thinks and how a computer computes. A brain is a continuously, highly parallelized computation-machine. In the brain, nothing ever takes a break from working. Neurons aren&rsquo;t paused because something else has to be processed. They work in parallel in a gigantic organic process, which results in human thinking and acting. The modern computer however, only ever processes a few things at any given time. Execution is handled by a C[entral]PU. I think it is therefore fundamentally impossible to create consciousness, with a digital machine which works in a mostly serial fashion. Similar to what John R. Searle suggests in his article &ldquo;Minds, Brains, and Programs&rdquo; where he introduced the chinese room experiment <a href="#SEAJ00">[SEAJ00]</a>.</p>

<h2 id="on-thinking-machines">On Thinking Machines</h2>

<p>I would boil down the term &ldquo;thinking&rdquo; in an AGI system to the following abilities:</p>

<ul>
<li>Absorb new information.</li>
<li>Analyze information regarding its usefullness.</li>
<li>Discard useless information.</li>
<li>Combine information to generate new original information or actions.</li>
</ul>

<p>If an AGI system has these abilities, it can turn into an ever self improving system that will eventually turn into an ASI system.</p>

<h2 id="what-is-intelligence">What is intelligence?</h2>

<p>Thinking and intelligence are very closely related, if not the same. I would differe the two by adding the aspect of rationality. Acting upon information in a rational fashion, to most efficiently satisfy ones needs.</p>

<p>And if we create a system which thinks and acts intelligently, we have essentially created an AGI.</p>

<h2 id="but-what-about-consciousness">But What About Consciousness?</h2>

<p>My definition of intelligence implies that an ASI system works towards fulfilling certain needs, or in other words: It has to fulfill a certain task or goal.
As shown before, consciousness is not part of an intelligent machine. It&rsquo;s all about processing information and acting upon it. What would it help the machine anyway to know that it is? I see no use in that and therefore don&rsquo;t think that it is required.</p>

<h2 id="asi-systems-also-require-ethics-don-t-they">ASI Systems Also Require Ethics, Don&rsquo;t They?</h2>

<p>So, an ASI would act purely rationally towards achieving its goals. This has some scary implications. I suggest reading the example of Turry the letter-signing robot <a href="#URBT01">[URBT01]</a> by Tim Urban. The robot of a rather unethical behaviour, by killing all humans.</p>

<h3 id="on-ethics-and-good-behaviour">On Ethics and Good Behaviour</h3>

<p>Where do ethics come from? I think it has to do with the fact that we as humans have the goal to survive as a species programmed into us. Everything that helps working towards that goal is something worth doing and everything that could prevent us from surviving as a species should obviously not be done.
Due to the evolutionary process, most of humans subconsciously work towards that goal.
Generally speaking we are nice to each other, because we want to be treated nicely. Being treated nicely improves our lives ever so slightly and therefore increases our chance to survive a tiny bit aswell. And if one human has a higher chance of surviving, the entire species has a higher chance of surviving. This is of course overly simplified, however I can&rsquo;t think of any examples in life, where I can&rsquo;t apply this thought process.</p>

<p>What I take away from this is, that it is extremely important for an ASI to have the right goal. This goal has to make sure that it makes sense for the ASI to let other species live their natural lives. (I include humans in &ldquo;other species&rdquo;.) Setting this goal is probably the most difficult task in the whole process. It requires the ones who set the goal to think like the ASI would and think through many different scenarios which could possibly occur. Forgetting any aspect to the implications of such a goal could be fatal to the entire human species. Or all species, to be more precise. If however we&rsquo;d manage to execute this step correctly, it would be the most important, most revolutionary invention. It would have a bigger and better impact on our lives than anything that we or any human in the past ever experienced.</p>

<h1 id="the-human-factor">The Human Factor</h1>

<p>I have to admit that I don&rsquo;t have enough trust in humans to get it right. A rush towards such a system can be observed, large nations trying to beat each other to developing an ASI. Imagining the potential power an entity, who first develops such a system would have, is slightly worrying, to say the least.</p>

<p>Considering that we possibly only have one single chance at getting this thing right, it is even more worrying that there is a rush towards it. The people developing this should really take their time to make sure they get it right. Because if they don&rsquo;t, the human species might just go extinct within a short amount of time.</p>

<h1 id="so-now-what">So Now What?</h1>

<p>So much could potentially go wrong, one might question whether the creation of an ASI is a good idea or not. So let&rsquo;s look at the possible outcomes. In the first scenario we assume that no ASI will ever be created, because everyone agrees that it is too dangerous and shouldn&rsquo;t be done. Let&rsquo;s ignore the fact that with the technology being easily available, someone would eventually do it anyway.</p>

<p>In such a scenario, our progress would be comparatively slow and I don&rsquo;t think we have this much time left to spread across other planets. If we want any chance at all at surviving, we need to spread into the solar system and in general, the further the better. We don&rsquo;t really know what is coming at us. What we do know, however, is that there are things coming at us that we cannot defend ourselves agianst. It&rsquo;s a question of time until another huge asteroid will hit earth. So unless we make our species somewhat more redundant, we will eventually go extinct.</p>

<p>So with in a scenario where an ASI exists, we have one additional option and that is that it could help us overcome these issues. Yes, one of the dangers is that we are possibly creating our own version of the dinosaur hoopla, but without trying, this is bound to happen eventually either way.</p>

<p>This to me is reason enough to at least give it a good try. That&rsquo;s the least we can do. Either we go extinct, or we possibly won&rsquo;t go extinct. So let&rsquo;s give ourselves the chance to survive, at least a little longer.</p>
			
			<h1 id="references">References</h1>
<table>
	<tbody>
		
		<tr>
			<td>[<a name="FORK00"></a>FORK00]</td>
			<td>K. Ford, et al., &ldquo;Cognitive Orthoses: Toward Human-Centered AI&rdquo;, <em>AI Magazine</em>, vol. 36, no. 4, pp. 5-8, 2015

			</td>
		</tr>
		
		<tr>
			<td>[<a name="URBT00"></a>URBT00]</td>
			<td>T. Urban.
(2015). The AI Revolution: The Road to Superintelligene [Online]. Available: <a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html</a>


			</td>
		</tr>
		
		<tr>
			<td>[<a name="URBT01"></a>URBT01]</td>
			<td>T. Urban.
(2015). The AI Revolution: Our Immortality or Extinction [Online]. Available: <a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html">https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html</a>


			</td>
		</tr>
		
		<tr>
			<td>[<a name="SEAJ00"></a>SEAJ00]</td>
			<td>J. Searle, &ldquo;Minds, Brains, and Programs&rdquo;, <em>THE BEHAVIORAL AND BRAIN SCIENCES</em>, vol. 3,  pp. 417-457, 1980

			</td>
		</tr>
		
	</tbody>
</table>

			
		<section class="disqus" >
			<div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "dujoram" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
		</section>
		</main>
		<footer>
			Copyright &copy; 2017 Joram S. Liebeskind
		</footer>
	</body>
</html>


