---
title: "P01 Reading Assignement"
date: 2017-09-26T00:01:50+02:00
draft: true
type: "post"
references:
  - Type: "journal"
    Ref: "FORK00"
    Title: "Cognitive Orthoses: Toward Human-Centered AI"
    Journal: "AI Magazine"
    Volume: 36
    Number: 4
    Authors:
      - "Kenneth M. Ford"
      - "Patrick J. Hayes"
      - "Clark Glymour"
      - "James Allen"
    Year: 2015
    Month:
    PageFirst: 5
    PageLast: 8
  - Type: "web"
    Ref: "URBT00"
    Title: "The AI Revolution: The Road to Superintelligene"
    Authors:
      - "Tim Urban"
    Year: 2015
    Available: "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"
  - Type: "web"
    Ref: "URBT01"
    Title: "The AI Revolution: Our Immortality or Extinction"
    Authors:
      - "Tim Urban"
    Year: 2015
    Available: "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html"
  - Type: "journal"
    Ref: "SEAJ00"
    Title: "Minds, Brains, and Programs"
    Journal: "THE BEHAVIORAL AND BRAIN SCIENCES"
    Volume: 3
    PageFirst: 417
    PageLast: 457
    Authors:
      - "John R. Searle"
    Year: 1980
---

Artificial intelligence might just be the next big thing that the human species will come up with. It might also be the last thing that it will ever create. Or will ever do. We might all die and everything will be over. Alternatively, we might just get lucky and AI will turn us into the immortal multiplanetary species that we all (or at least some of us) dream of.
<!--more-->


# Disclaimer
At the time of writing, I am far from being an expert on this topic and everything you're about to read is pure speculation, mostly based on a two-part blog post series with the titles "The AI Revolution: The Road to Superintelligence" and "The AI Revolution: Our Immortality or Extinction" by Tim Urban [[URBT00]({{< relref "#URBT00" >}}), [URBT01]({{< relref "#URBT01" >}})], as well as an article from AI Magazine with the title "Cognitive Orthoses: Toward Human-Centered AI" [[FORK00]]({{< relref "#FORK00" >}}).

I wrote this blog post in the context of an AI class assignement, which consistet of, in a first step reading the referenced material and thereafter draw conclusion in the form of a post.

<!--# When Birds Need Runways
I quite like the analogy, which K. Ford et. al. introduce. They compare the relation of Artificial Flight (AF) to birds to the relation of Artificial Intelligence (AI) to real intelligence. Only when the Wright brothers started asking the right questions, namely about lift and thrust, instead of trying to exactly mimic birds including the construction of fethers and the mechanics of the flapping of the wings, AF became possible. [[FORK00]]({{< relref "#FORK00" >}}).

After that realization, development of AF started to focus on what's relevant: efficient flight, made for humans. After all, there is no need for an artificial bird (plane) to land on trees.

This suggests that, maybe, trying to copy the human brain in order to create an Artificial General Intelligence (AGI) would not lead to a desirable result, if any at all. This however doesn't mean, that it wouldn't be worth doing.

# Could it Possibly Fly?
And by "fly" I mean, will there be an ASI. I say yes. Well, not me, but the many scientists I trust, they think so. Whether AGI and therefore Artificial Super Intelligence (ASI) will happen or not, is, in many AI scientists minds, out of question. There is no way around it. Someone will eventually come up with an AGI system and the logical conclusion, keeping in mind the exponential growth of power, that an AGI will turn itself into an ASI. [[URBT00]]({{< relref "#URBT00" >}})
-->


# Introduction / What I read
<!--
1. Exponential development of technologies
2. Inevitebility of AGI -> ASI
3. Nano-Tech.
4. Implications of Nano-Bots with ASI
-->

## What I read
Progress increases more and more each day. Many fields in science experience an exponential growth. It is hard to understand what effects this has on our daily lives and even more challanging to imagine how this will effect our future. Arguably the most important development is the increase in computational capabilities. According to Tim Urban, computational power equivalent to a human brains ability, will be widely spread and affordable. Expensive, but affordable.
Reaching this milestone, will enable humans to simulate brains. It is safe to assume that we will eventually reach this point.
Then the question is, how to create an Artificial General Intelligence (AGI). That is, an AI which can absorb and interprete new knowledge, connect it with previously learned knowledge and apply it to arbitrary problems, which were either imposed onto itself or by other humans. For an AI to qualify as an AGI, this process has to reach a quality roughly equivalent to what human brain is capable of achieving.
In the process of gathering new knowledge, it is only reasonable for an AGI to improve itself. This implies that it would rapidly reach a point, where it would be capable of mastering tasks and developing technologies far beyond our understanding at a rate which we couldn't keep up with. We'd have an Artificial Super Intelligence (ASI), which created itself, simply because an AGI strives to become more intelligent. [[URBT00]]({{< relref "#URBT00" >}})
This is a very rough summary of Tim Urbans blog posts and only touches the surface, following I want to focus on my thoughts on different fascetes of the scenario.

# Potentially good stuff / Implications
<!--
1. Create almost anything
2. Would survive as species
3. Would survive as individuals
------
-->

Tim Urban describes how this could either make the human species immortal or extinguish it. Let's assume that we created a good ASI. A super intelligence that acts in the interest of us humans, that acts the way we tell it to and understands and supports our intents. The implications of this would be huge.

We'd have a tool that could find a solution to almost any problem. As Tim Urban suggests, we'd find ways to stop ageing and create a world where natural death would only occure if one would choose it. Almost any disease could be healed, death among humans would be a rare occurance.

But more importantly, we'd find ways to survive as a species. I personally think, the only truly viable way to achieve that, is by becoming a multiplanetary species. No matter how good we get at surviving as humans on earth, there are dangers out there which outscale all possible solutions we could come up with in the limitations of our earth, or even our solar system. The second driving force of exploring such options is overpopulation. If we simply stop dieing, we need more places to live at and I'm quite convinced that the colonialization of other planets is more or less the only solution to this challange.

Ultimately, if done correctly, I'm certain that the creation of an ASI would drive us towards this state rather quickly, much quicker than one would intuitively think would be possible. Reaching this point would ensure the survival of our species for a very, very long time and with the potentially extreme progression of all fields in science, a new and better way to live beyond what we could possibly imagine.

# Dangers and what I think would be the way to get there
<!--
1. How we think of AI vs. How it is more likely to turn out
   - Purely Rational
   - On self-consciousness
   - (Maybe chinese-room experiment)
   - What is intelligence anyway?
2. Setting goals correctly from the start
3. True cooperation between developing parties
4. Better understand the dangers of AI, then prevent those from happening.

---
-->

In this section I want to discuss the idea of consciousness and human-like behaviour in AI system. Or rather, what the lack thereof implies.

At this point, I would also like to remind you of the "Disclaimer" section at the beginning of this blog post. I am in no way qualified to talk about the following topics and therefore statements might be completely inaccurate and/or just wrong.


## Will AI have consciousness?
This is a question that cannot be answered easily. If we were to say, that any program which passes the Turing test is conscious, then this would mean that an AGI or an ASI would therefore also be conscious, since it would be smart enough to pass it. I however don't think that this is a measurement accurate enough. Firstly, we'd need to understand what consciousness is, for how can we make a statement about how to create something if we don't know how it works. (Though this claim stands in direct contradiction to how nature works, where the most intelligent things are created with very few, if any, intellectual thought.)

Another thing is the fundamental difference between how the human brain thinks and how a computer computes. A brain is a continously, highly parallelized computation-machine. In the brain, nothing ever takes a break from working. Neurons aren't paused because something else has to be processed. They work in parallel in a gigantic organic process which results in human thinking and acting. The modern computer however, only ever processes a few things at any given time. Execution is handled by a C[entral]PU. I think it is therefore fundamentally impossible to create consciousness, with a digital machine which works in such a fassion. Similar to what John R. Searle suggests in his article "Minds, Brains, and Programs" where he introduced the chinese room experiment [[SEAJ00]]({{< relref "#SEAJ00" >}}).

## What does an AGI/ASI system need to think?
I would boil down the term "thinking" in an AGI system to the following abilities:

- Absorb new information.
- Analyze information regarding its usefullness.
- Discard useless information.
- Combine information to generate new original information or generate actions.

If an AGI system has these abilities, it can turn into an ever self improving system that will eventually turn into an ASI system.

## What is intelligence?
Thinking and intelligence are very closely related, if not the same. I would differe the two by adding the aspect of rationality. Acting upon information in a rational fashion, to most efficiently satisfy ones needs.

And if we create a system which thinks and acts intelligently, we have essentially created an AGI.

## But wait, what about consciousness?
My definition of intelligence implies that an ASI system works towards fulfilling certain needs, or in other words: It has to fulfill a certain task or goal.
As shown before, consciousness is not part of an intelligent machine. It's all about processing information and acting upon it. What would it help the machine anyway to know that it is? I see no use in that and therefore don't think that it is required.

## How do we make sure, that such an ASI will act ethically?
So, an ASI would act purely rationally towards achieving its goals. This has some scary implications. I suggest reading the example of Turry the letter-signing robot [[URBT01]]({{< relref "#URBT01" >}}) by Tim Urban. The robot of a rather unethical behaviour, by killing all humans.

### Thoughts on ethics and good behaviour
Where do ethics come from? I think it has to do with the fact that we as humans have to goal to survive as a species programmed into us. Everything that helps working towards that goal is something worth doing and everything that could prevent us from surviving as a species should obviously not be done.
Due to the evolutionary process, most of humans subconciously work towards that goal.
We are generally speaking nice to others, because we want to be treated nicely. Being treated nicely improves our lives ever so slightly and therefore increases our chance to survive a tiny bit aswell. And if one human has a higher chance of surviving, the entire species has a higher chance of surviving. I can't think of any examples in life, where I can't apply this thought process.

What I take away from this is, that it is extremely important for an ASI to have the right goal. This goal has to make sure that it makes sense for the ASI to let other species live their natural lives. (I include humans in "other species".) Setting this goal is probably the most difficult task in the whole process. It requires the ones who set the goal to think like the ASI would and think through many different scenarios which could possibly occur. Forgetting any aspect to the implications fo such a goal could be fatal to the entire human species. Or all species, to be more precies. If however we'd manage to execute this step correctly, it would be the most important, most revolutionary invetion. It would have a bigger and better impact on our lives than anything that we or any human in the past ever experienced.


# TITLE PENDING
I have to admit that I don't have enough trust in humans to get it right. Not because it wouldn't be possible. A rush towards such a system can be observed, large nations trying to beat each other to developing an ASI. Imagining the potential power an entity, who first develops such a system would have, is slightly worrying, to say the least.

Considering that we possibly only have one single chance at getting this thing right, it is even more worrying that there is a rush towards it. The people developing this should really take their time to make sure they get it right. Because if they don't, the human species might just go extinct within a short amount of time. And that's most of us, I hope, wouldn't want to happen.


# Conclusion
Despite the dangers of an ASI, I still think it should be created, if it turns out to actually be possible. We as humans will eventually go extinct, this is just a question of time. With an ASI, we might be able to prevent this. So it's worth taking a shot at it, if it does go wrong, while it would be extremely unfortunate, at least we could say we tried.
