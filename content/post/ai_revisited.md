---
title: "AI Revisited"
date: 2017-12-19T12:33:58+01:00
draft: true
type: "post"
references:
  - Type: "web"
    Ref: "URBT00"
    Title: "The AI Revolution: The Road to Superintelligene"
    Authors:
      - "Tim Urban"
    Year: 2015
    Available: "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"
  - Type: "web"
    Ref: "URBT01"
    Title: "The AI Revolution: Our Immortality or Extinction"
    Authors:
      - "Tim Urban"
    Year: 2015
    Available: "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html"
  - Type: "web"
    Title: "The Seven Deadly Sins of Predicting the Future of AI"
    Ref: "BROR00"
    Year: 2017
    Authors:
      - "Rodney Brooks"
---

About two months ago, I wrote my previous blog post on ANI, AGI and ASI as part of an assignment in an AI course I'm attending. This is a follow up post, which is written to complete the second part of that assignment. After reading a more critical blog post on the topic [BROR00]({{< relref "#BROR00" >}}) and after a semester of learning about AI, this shall be a follow up to my previous blog post.

<!--more-->

# Disclaimer
I like disclamiers. They let me speak more freely and let you understand my biases. Most importantly though, they let you know how (un)trustworthy the information I put here is.
I encourage you to read to read the disclaimer I put at the top of my previous blog post, since it still holds true. There is a tiny difference, however: I now have a slightly better impression of the field of AI, as we were given a solid overview of the field during the last semester at the AI course. Also I've worked on a project wich tried to improve the training of Generative Adversarial Networks, a specific kind of Artificial Neural Nets.

# ASI is not Worth Thinking or Worrying About
I strongly disagree. Rodney Brooks proposes the concept, that technology often doesn't develop exponentially, but rather follows an S curve, meaning it develops quickly, then disappoints and only later becomes established. He also shows, with the example of the iPods storage over several generations, that it did not increase exponentially.[BROR00]({{< relref "#BROR00" >}})

I see several flaws in such an argument. What is the use of several terabytes of storage on any music player. There's no way one could go through this amount of music within a lifetime (claim without any grounds, but I'm quite sure of this.) So indeed, most music players storage did not increase exponentially. But how is this a measure of how AI might develop? It isn't, there's no connection from iPod storage to AI.
He says that AI technologies are very narrow and not at all close to general and that I have to agree with. There doesn't seem to be any general, or subset of general, AI technology around.

Where R. Brooks completely loses me though, is when he started speaking about Moore's Law not being an actual law. Of course it isn't. Nobody claimed that it is an actual law that would be comparable to the laws of physics. It's just a prediction once made that held true for much longer than once thought possible.

Oh and on "Brains are just too complicated to recreate, we might just be too stupid" (Not a direct quote). Right now they apparently are. But brains do exist, right? I'm quite sure they are. So they can come in to existance. Therefore there is a way to create them. Therefore it is extremely likely that there is a way to artificially recreate or simulate them. "We might be too stupid, so we shouldn't try" is not a philosophy science should or does follow. That's something we can leave to the curch.

# The ASI Hypothesis
And that leads me to the point I want to make. Is ASI going to happen? I think so. If no other disaster happens to humanity which would prevent it from happening, it probably will happen. It's a question of time. Maybe it might not happen quite as soon as has been predicted by many and probably it requires technologie that have not been invented yet, but those are no arguments against the possibilities of AI. Either way, ASI is a hypothesis and it seems highly likely, but I doubt anyone is claiming that it absolutely for sure is going to happen. And I'm not claiming so either, it just seems very likely to me.

## Let's Maybe do Worry About ASI
And because ASI seems so likely to happen, due to the many reasons Tim Urban states in his blog posts [[URBT00]({{< relref "#URBT00" >}}), [URBT01]({{< relref "#URBT01" >}})] I mostly stand by what I've written in my previous post.

My opinions only changed in how far I think we have progressed. There doesn't seem to any technology, capable of developing into an AGI. But that doesn't mean that the potential problems of it should not be discussed. It is important to develop solutions for these challanges, before it is too late, before there is an AGI that behaves unpredictably. (I'd like to referr to the paper clip story, which R. Brooks claims to be unrealistic without providing any convincable argument, except of saying that "it is stupid".)

# Concluding thoughts.
So I stand by my point that developing safety strategeies around ASI is important and has to be done before an ASI is developed. Saying that it porbably won't happen, which is also true, there's always a probability of something not being the case if there's a probability for it to be the case, doesn't mean at all that the whole subject should be dismissed as silly. The implications of ASI are huge and it is indeed extremely difficult to predict, what an ASI could potentially lead to, so let's figure those things out before it happens, even if there's a chance that it won't happen.
